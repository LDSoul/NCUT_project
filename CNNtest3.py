# -*- coding: utf-8 -*-
"""CNN測試3.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1y2ffd1QCGhYqspLQ1mLG18_A4fGNe-dg
"""

#layer
import numpy as np

def softmax(x):
    x = x - np.max(x, axis=-1, keepdims=True) #防止溢位 keepdims = True 用途是維持原維度

    # 以下執行 softmax
    # exp_x / sum_exp_x 
    return np.exp(x) / np.sum(np.exp(x), axis=-1, keepdims=True)

def cross_entropy_error(y, t):
    '''
    :t 為一般數值 非 one- hot- encoding
    :y 上一節點輸出數值
    '''
    if y.ndim == 1: # 將維度轉為一維
        t = t.reshape(1, t.size)
        y = y.reshape(1, y.size)
        
    if t.size == y.size: # 尋找最大值索引
        t = t.argmax(axis=1)
             
    batch_size = y.shape[0]

    # 以下執行 cross entropy error
    # -sum(tnk * log(ynk)) / N
    # n => batch_num, k => t, N => batch_size
    return -np.sum(np.log(y[np.arange(batch_size), t] + 1e-7)) / batch_size

def im2col(input_data, filter_h, filter_w, stride=1, pad=0): # image to column 
    '''
    :input_data 輸入資料(四維, N, C, H, W) #改為 N W H C
    :filter_h 濾鏡高
    :filter_w 濾鏡寬
    :stride 步幅
    :pad 向各維度向外填補寬度
    '''
    N, C, H, W = input_data.shape #C 與 W 兌換
    '''
    :N : batch_num  實際 N 545 
    :C : channel         H 100  C 與 W 兌換 我的是batch_num H W channel 跟你的不一樣
    :H : height          W 100
    :W : width           C 1
    '''
    # 求出輸出高及寬
    out_h = (H + 2*pad - filter_h)//stride + 1
    out_w = (W + 2*pad - filter_w)//stride + 1

    img = np.pad(input_data, [(0,0), (0,0), (pad, pad), (pad, pad)], 'constant') # 向各維度填補(batch_num 與 channel 維度無須填補)
    col = np.zeros((N, C, filter_h, filter_w, out_h, out_w)) # 設定輸出大小

    for y in range(filter_h):
        y_max = y + stride*out_h
        for x in range(filter_w):
            x_max = x + stride*out_w
            col[:, :, y, x, :, :] = img[:, :, y:y_max:stride, x:x_max:stride] #error shape (100,100,96,1) into shape (100,1,96,96)
    '''
    將原 col 格式為(N, C, filter_h, filter_w, out_h, out_w)
    轉換為 (N, out_h, out_w, C, filter_h, filter_w)
    並將輸出大小轉換為二維形式(N*out_h*out_w, C*filter_h*filter_w)
    '''
    col = col.transpose(0, 4, 5, 1, 2, 3).reshape(N*out_h*out_w, -1)
    return col


def col2im(col, input_shape, filter_h, filter_w, stride=1, pad=0):# column to image 卷積層反向傳播時用
    '''
    :col 從正向傳播輸出的 column
    :input_shape 這裡只 input_data 的形狀
    :filter_h 濾鏡高
    :filter_w 濾鏡寬
    :stride 步幅
    :pad 向各維度向外填補寬度
    '''
    N, C, H, W = input_shape
    '''
    :N : batch_num
    :C : channel
    :H : height
    :W : width
    '''
    # 求出輸出高及寬
    out_h = (H + 2*pad - filter_h)//stride + 1
    out_w = (W + 2*pad - filter_w)//stride + 1
    '''
    將col形狀轉換為原六維的形式(N, out_h, out_w, C, filter_h, filter_w)
    再將原格式為(N, out_h, out_w, C, filter_h, filter_w)
    轉回為原(N, C, filter_h, filter_w, out_h, out_w)
    '''
    col = col.reshape(N, out_h, out_w, C, filter_h, filter_w).transpose(0, 3, 4, 5, 1, 2)
    
    img = np.zeros((N, C, H + 2*pad + stride - 1, W + 2*pad + stride - 1)) # 設定輸出大小
    for y in range(filter_h):
        y_max = y + stride*out_h
        for x in range(filter_w):
            x_max = x + stride*out_w
            img[:, :, y:y_max:stride, x:x_max:stride] += col[:, :, y, x, :, :]

    return img[:, :, pad:H + pad, pad:W + pad]

class Relu:
    def __init__(self):
        self.mask = None

    def forward(self, x):
        '''
        f(x) = x as (x > 0) + 0 as (x <= 0)
        '''
        self.mask = (x <= 0)
        out = x.copy()
        out[self.mask] = 0

        return out

    def backward(self, dout):
        '''
        dx = 1 as (x > 0) + 0 as (x <= 0)
        '''
        dout[self.mask] = 0
        dx = dout

        return dx

class Affine:
    def __init__(self, W, b):
        self.W =W
        self.b = b
        
        self.x = None
        self.original_x_shape = None

        self.dW = None
        self.db = None

    def forward(self, x):
        self.original_x_shape = x.shape # 反向傳播時須將輸出資料轉換為大小用
        x = x.reshape(x.shape[0], -1) #轉為二維陣列[資料數量,資料]
        self.x = x

        out = np.dot(self.x, self.W) + self.b # out = x˙W + b

        return out

    def backward(self, dout):
        '''
        以下執行的步驟就是與正向傳播時完全相反
        (須注意向量大小)
        '''
        dx = np.dot(dout, self.W.T)
        self.dW = np.dot(self.x.T, dout)
        self.db = np.sum(dout, axis=0)
        
        dx = dx.reshape(*self.original_x_shape) # 最後將輸出轉換為正向傳播時輸入大小
        return dx

class SoftmaxWithLoss:
    '''
    因為大多輸出層時 softmax 常與 cross entropy error 在一起
    又加上神經網路在推論時只對評分的最大值有興趣 因此無須sofmax
    但 神經網路學時時會需要 因此這裡把 softmax 與 cross entropy error
    合併在一起
    '''
    def __init__(self):
        self.loss = None
        self.y = None
        self.t = None

    def forward(self, x, t):
        self.t = t
        self.y = softmax(x)
        self.loss = cross_entropy_error(self.y, self.t)
        
        return self.loss

    def backward(self, dout=1):
        batch_size = self.t.shape[0]
        '''
        若 y 與 t 大小相同則 (y - t) / batch_size
        y 與 t 大小不同時 將 y 內取 正確數值  內值 -= 1
        '''
        if self.t.size == self.y.size:
            dx = (self.y - self.t) / batch_size
        else:
            dx = self.y.copy()
            dx[np.arange(batch_size), self.t] -= 1
            dx = dx / batch_size
        
        return dx

class Convolution:
    def __init__(self, W, b, stride=1, pad=0):
        self.W = W
        self.b = b
        self.stride = stride
        self.pad = pad
        
        self.x = None   
        self.col = None
        self.col_W = None
        
        self.dW = None
        self.db = None

    def forward(self, x):
        FN, C, FH, FW = self.W.shape
        '''
        :FN filter_num
        :C channel
        :FH filter height
        :FW filter width
        '''
        N, C, H, W = x.shape
        '''
        :N batch_num
        :C channel
        :H height
        :W width
        '''
        # 設定輸出大小
        out_h = 1 + int((H + 2*self.pad - FH) / self.stride)
        out_w = 1 + int((W + 2*self.pad - FW) / self.stride)

        col = im2col(x, FH, FW, self.stride, self.pad) # 將四維轉為二維
        col_W = self.W.reshape(FN, -1).T # 將濾鏡也轉為二維

        out = np.dot(col, col_W) + self.b # out = x˙W + b
        # 將輸出格式 (N, H, W, C) 轉為 (N, C, H, W)
        out = out.reshape(N, out_h, out_w, -1).transpose(0, 3, 1, 2)

        # 以下參數反向傳播用
        self.x = x
        self.col = col
        self.col_W = col_W

        return out

    def backward(self, dout):
        FN, C, FH, FW = self.W.shape
        '''
        :FN filter_num
        :C channel
        :FH filter height
        :FW filter width
        '''
        # 將格式 (N, C, H, W) 轉為 (N, H, W, C) 並將其轉為二維
        dout = dout.transpose(0,2,3,1).reshape(-1, FN)

        # x˙W + b 的反向傳導前 W, b 設定
        self.db = np.sum(dout, axis=0)
        self.dW = np.dot(self.col.T, dout)
        self.dW = self.dW.transpose(1, 0).reshape(FN, C, FH, FW)

        # 進行反向傳導
        dcol = np.dot(dout, self.col_W.T)
        # 將二維轉維四維形式
        dx = col2im(dcol, self.x.shape, FH, FW, self.stride, self.pad)

        return dx

class Pooling:
    def __init__(self, pool_h, pool_w, stride=1, pad=0):
        self.pool_h = pool_h
        self.pool_w = pool_w
        self.stride = stride
        self.pad = pad
        
        self.x = None
        self.arg_max = None

    def forward(self, x):
        N, C, H, W = x.shape
        '''
        :N batch_num
        :C channel
        :H height
        :W width
        '''
        # 設定輸出大小
        out_h = int(1 + (H - self.pool_h) / self.stride)
        out_w = int(1 + (W - self.pool_w) / self.stride)

        # 將四維轉為二維
        col = im2col(x, self.pool_h, self.pool_w, self.stride, self.pad) 
        col = col.reshape(-1, self.pool_h*self.pool_w) 

        arg_max = np.argmax(col, axis=1)
        out = np.max(col, axis=1)
        
        # 將輸出格式 (N, H, W, C) 轉為 (N, C, H, W)
        out = out.reshape(N, out_h, out_w, C).transpose(0, 3, 1, 2)

        # 以下參數反向傳播用
        self.x = x
        self.arg_max = arg_max

        return out

    def backward(self, dout):
        # 將輸出格式 (N, C, H, W) 轉為 (N, H, W, C)
        dout = dout.transpose(0, 2, 3, 1)

        # 將 dout 數值 設定成正向傳播最大向位置 其餘數值為零
        pool_size = self.pool_h * self.pool_w
        dmax = np.zeros((dout.size, pool_size))
        dmax[np.arange(self.arg_max.size), self.arg_max.flatten()] = dout.flatten()
        dmax = dmax.reshape(dout.shape + (pool_size,)) 
        
        dcol = dmax.reshape(dmax.shape[0] * dmax.shape[1] * dmax.shape[2], -1)

        # 將二維轉維四維形式
        dx = col2im(dcol, self.x.shape, self.pool_h, self.pool_w, self.stride, self.pad)
        
        return dx

#network
import numpy as np
from collections import OrderedDict


class CNN:
    def __init__(self, input_dim=(1, 100, 100), 
                 conv_param={'filter_num':30, 'filter_size':5, 'pad':0, 'stride':1},
                 hidden_size=100, output_size=3, weight_init_std=0.01):
        
        filter_num = conv_param['filter_num']
        filter_size = conv_param['filter_size']
        filter_pad = conv_param['pad']
        filter_stride = conv_param['stride']
        input_size = input_dim[1]
        conv_output_size = (input_size - filter_size + 2*filter_pad) / filter_stride + 1
        pool_output_size = int(filter_num * (conv_output_size/2) * (conv_output_size/2))

        # 參數設定
        self.params = {}
        self.params['W1'] = weight_init_std * \
                            np.random.randn(filter_num, input_dim[0], filter_size, filter_size)
        self.params['b1'] = np.zeros(filter_num)
        self.params['W2'] = weight_init_std * \
                            np.random.randn(pool_output_size, hidden_size)
        self.params['b2'] = np.zeros(hidden_size)
        self.params['W3'] = weight_init_std * \
                            np.random.randn(hidden_size, output_size)
        self.params['b3'] = np.zeros(output_size)

        # 神經網路建構
        self.layers = OrderedDict()
        self.layers['Conv1'] = Convolution(self.params['W1'], self.params['b1'],
                                           conv_param['stride'], conv_param['pad'])
        self.layers['Relu1'] = Relu()
        self.layers['Pool1'] = Pooling(pool_h=2, pool_w=2, stride=2)
        self.layers['Affine1'] = Affine(self.params['W2'], self.params['b2'])
        self.layers['Relu2'] = Relu()
        self.layers['Affine2'] = Affine(self.params['W3'], self.params['b3'])

        self.last_layer = SoftmaxWithLoss()

    def predict(self, x): # 預測數值
        for layer in self.layers.values():
            x = layer.forward(x)
        return x
    
    def accuracy(self,x,t):
        y = self.predict(x)
        y = np.argmax(y,axis=1)
        if t.ndim != 1:t = np.argmax(t,axis=1)

        acc = np.sum(y==t) / float(x.shape[0])
        return acc

    def loss(self, x, t): # loss function
        y = self.predict(x)
        return self.last_layer.forward(y, t)
        
    def gradient(self, x, t): # gradient ==> 進行反向傳播
        self.loss(x, t)

        dout = 1
        dout = self.last_layer.backward(dout)

        layers = list(self.layers.values())
        layers.reverse()
        for layer in layers:
            dout = layer.backward(dout)

        grads = {}
        grads['W1'], grads['b1'] = self.layers['Conv1'].dW, self.layers['Conv1'].db
        grads['W2'], grads['b2'] = self.layers['Affine1'].dW, self.layers['Affine1'].db
        grads['W3'], grads['b3'] = self.layers['Affine2'].dW, self.layers['Affine2'].db

        return grads

#優化器
import numpy as np

class Adam:
    def __init__(self, lr=0.001, beta1=0.9, beta2=0.999):
        self.lr = lr
        self.beta1 = beta1
        self.beta2 = beta2
        self.iter = 0
        self.m = None
        self.v = None
        
    def update(self, params, grads):
        # 設定 m 與 n 兩個參數
        if self.m is None: 
            self.m, self.v = {}, {}
            for key, val in params.items():
                self.m[key] = np.zeros_like(val)
                self.v[key] = np.zeros_like(val)

        # 以下進行 params 更新
        self.iter += 1
        lr_t  = self.lr * np.sqrt(1.0 - self.beta2**self.iter) / (1.0 - self.beta1**self.iter)         
        
        for key in params.keys():
           
            self.m[key] += (1 - self.beta1) * (grads[key] - self.m[key])
            self.v[key] += (1 - self.beta2) * (grads[key]**2 - self.v[key])
            
            params[key] -= lr_t * self.m[key] / (np.sqrt(self.v[key]) + 1e-7)

class SGD:
    def __init__(self, lr=0.01):
        self.lr = lr
        
    def update(self, params, grads):
        # 進行 aams 更新
        for key in params.keys():
            params[key] -= self.lr * grads[key]

#Training
import numpy as np
import pandas as pd
from keras.utils.np_utils import to_categorical
from sklearn.model_selection import train_test_split

#=====讀CSV檔案====================================================================================================
train = pd.read_csv("train.csv")
test = pd.read_csv("test.csv")

#取得csv內 label那行資料
t_train = train["label"]

x_train = train.drop(labels = ["label"],axis = 1) 

x_train.isnull().any().describe()
test.isnull().any().describe()

#正規化
x_train = x_train / 255.0
test = test / 255.0

#轉為之後訓練神經網路 所需要的維度
x_train = x_train.values.reshape(-1,1,100,100)
test = test.values.reshape(-1,1,100,100)

#原本矩陣直的 然後把它變成矩陣形狀 共3個寶可夢
t_train = to_categorical(t_train,num_classes = 3) 

random_seed = 1 #種子碼 隨便數字

#train_test_split將會自動把資料分類為 x_train, x_test, y_train, y_test 這四種
#測試資料佔的比例暫訂為20%, 因此test_size = 0.2
x_train, x_test, t_train, t_test = train_test_split(x_train, t_train, test_size = 0.2, random_state=random_seed)

#print(x_train.shape)
#print(t_train.shape)
#===============================================================================================================


network = CNN(input_dim=(1,100,100), 
                        conv_param = {'filter_num': 30, 'filter_size': 5, 'pad': 0, 'stride': 1},
                        hidden_size=100, output_size=3, weight_init_std=0.01)

train_size = x_train.shape[0]
batch_size = 100
learning_rate = 0.001
optimizer = Adam(learning_rate)
train_acc_list = []
test_acc_list = []
iter_per_epoch = max(550 / batch_size, 1) #實際測資為545 在此把側資當550筆 使epoch為整數

for i in range(200):
    # 取得小批次
    batch_mask = np.random.choice(train_size, batch_size)
    x_batch = x_train[batch_mask]
    t_batch = t_train[batch_mask]

    # 設定梯度
    grads = network.gradient(x_batch, t_batch)
    # 設定優化器(更新參數用)
    optimizer.update(network.params, grads)

    # 顯示 loss 的變化
    loss = network.loss(x_batch, t_batch)
    print(str(i) + ':' + str(loss))
    
    # 顯示 acc
    if i % iter_per_epoch == 0:
      train_acc = network.accuracy(x_train, t_train)
      test_acc = network.accuracy(x_test,t_test)
      train_acc_list.append(train_acc)
      test_acc_list.append(test_acc)
      #print(train_acc,test_acc)

#test
import numpy as np
import matplotlib.pyplot as plot
import matplotlib.pyplot as plt

def NumToName(x): #與資料內(.csv) label對應之神奇寶貝
  if x==0:
    return 'Pika' #皮卡丘
  elif x==1:
    return 'Dragonite' #快龍
  elif x==2:
    return 'Ekans' #阿伯蛇
for i in range(5):
  plt.subplot(1,5,i+1) #多合一顯示
  rnd = np.random.randint(1,168,1)  #測資只有168張三隻神奇寶貝圖
  ans = network.predict(np.array(test[rnd])) # 神經網路預測
  ans = np.argmax(ans, axis = -1) # 取出最大值位置(即為神經網路預測數值)
  x = NumToName(ans)
  plt.title(x)
  plt.imshow(test[rnd].reshape(100, 100))

print('train_acc:' , train_acc,'  test_acc:',test_acc) #成功率顯示

